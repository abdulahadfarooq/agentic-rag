Agentic RAG with CrewAI and Ollama

This repository implements an Agentic RAG (Retrieval-Augmented
Generation) pipeline using various technologies like CrewAI, Ollama,
LlamaIndex, pgvector, and Docling to provide a complete, end-to-end
solution for querying and agent-driven response generation.

Overview

This project leverages the following technologies to create an
intelligent agent-based RAG stack:

pgvector: For storing and querying embeddings in PostgreSQL.

LlamaIndex: For the embedding pipeline, responsible for indexing and
retrieving documents.

Ollama: For hosting large language models that are used for both
embeddings and generation.

CrewAI: For orchestrating agent tasks, including retrieval, response
writing, and evaluation.

Phoenix: For monitoring and logging the performance of the agents.

Docling: For handling document uploads (PDF, DOCX, etc.), converting
them to text using OCR and structured document processing.

FastAPI: To expose endpoints for querying and agent interaction through
Swagger UI.

File Structure

The project is structured as follows:

agentic-rag/ ├── docker-compose.yml \# Docker Compose configuration to
set up services ├── .env.example \# Example environment variables ├──
.gitignore \# Git ignore file ├── backend/ │ ├── Dockerfile \#
Dockerfile to build backend service │ ├── requirements.txt \# Python
dependencies │ └── app/ │ ├── **init**.py │ ├── config.py \#
Configuration for the app (e.g., environment settings) │ ├── db.py \#
Database connection setup │ ├── services/ │ │ ├── indexing.py \#
Indexing logic using LlamaIndex and pgvector │ │ ├── retrieval.py \#
Retrieval logic (querying documents) │ │ ├── doc_parse.py \# Docling
parser for handling file uploads (PDF, DOCX) │ ├── agents/ │ │ ├──
tools.py \# CrewAI tool functions (e.g., query_rag, cite_chunks) │ │ └──
crew.py \# CrewAI agent definition and orchestration │ ├── routers/ │ │
├── health.py \# Health check route │ │ ├── ingest.py \# Endpoint for
file uploads and document ingestion │ │ ├── query.py \# RAG query
endpoint │ │ └── agents.py \# Endpoint for agentic RAG tasks │ └──
main.py \# FastAPI app entry point ├── .gitignore \# Ignore unnecessary
files in the repo └── README.md \# This file

Services Used 1. pgvector (PostgreSQL with pgvector extension)

pgvector is a PostgreSQL extension used to store vector embeddings. It
allows efficient storage and querying of embeddings, which is crucial
for RAG workflows.

Service: pgvector/pgvector:pg16

Purpose: Store document embeddings generated by LlamaIndex.

2.  LlamaIndex

LlamaIndex is used for creating and querying the index of documents. It
integrates with pgvector to store and retrieve embeddings from the
PostgreSQL database.

Service: Integrated into the backend service.

Purpose: Handle the embedding pipeline for document ingestion and
retrieval.

3.  Ollama

Ollama hosts large language models (LLMs) and provides an API for
running inference tasks. It supports various models and is used for both
embeddings and generation.

Service: ollama/ollama:latest

Purpose: Host and serve models for text generation and embeddings.

4.  CrewAI

CrewAI is used to define agents that handle complex tasks like querying,
summarizing, and evaluating results. The agents operate sequentially to
retrieve and generate responses based on user queries.

Service: Integrated into the backend service.

Purpose: Orchestrate agents responsible for research, writing, and
evaluating responses.

5.  Phoenix

Phoenix is a tool for observability and logging. It tracks the
performance of agents and processes, helping to monitor and evaluate the
RAG pipeline.

Service: arizephoenix/phoenix:latest

Purpose: Monitor and log the agent workflows.

6.  FastAPI (API Endpoints)

FastAPI exposes the endpoints for interacting with the system. These
include endpoints for querying the RAG system, running agents, and
ingesting documents.

Service: Integrated into the backend service.

Purpose: Expose REST API endpoints and provide Swagger UI for
interaction.

7.  Docling

Docling is used to handle document uploads, such as PDFs and DOCX files.
It extracts text from these documents and converts them into a format
suitable for processing in the RAG pipeline.

Service: Integrated into the backend service.

Purpose: Convert and parse documents (PDF, DOCX) into text, enabling
content ingestion.

Setup and Installation

Follow these steps to set up and run the entire stack using Docker.

Prerequisites

Docker and Docker Compose should be installed on your system.

Step 1: Clone the repository Clone this repository

Step 2: Build and Start the Docker Containers

Run the following command to build and start all the necessary services
using Docker Compose.

docker-compose up --build

This will start the following services:

PostgreSQL with pgvector

Ollama (for model hosting)

Phoenix (for observability)

Backend (FastAPI, CrewAI, LlamaIndex)

OpenWebUI (for interacting with the models)

The services will be available on the following ports:

Backend API: http://localhost:8000

Swagger UI: http://localhost:8000/docs

Phoenix UI: http://localhost:6006

OpenWebUI: http://localhost:8080

Step 3: Load the Models in Ollama (if not already done)

Once the containers are running, you may need to pull the necessary
models for Ollama:

docker exec -it rag_ollama ollama pull llama3.2:1b docker exec -it
rag_ollama ollama pull nomic-embed-text

These models will be used for both embeddings and generation.

API Endpoints 1. Health Check

To check if the backend service is running correctly, visit:

GET http://localhost:8000/health

Response:

{ "ok": true }

2.  Ingest Files

To upload documents (e.g., PDFs, DOCX) and add them to the index, visit:

POST http://localhost:8000/ingest/files

Request (example using curl):

curl -X POST http://localhost:8000/ingest/files -F
"files=@/path/to/your/file.pdf"

3.  Normal RAG Query

To perform a normal RAG query, visit:

POST http://localhost:8000/query

Request:

{ "prompt": "Tell me about the best practices for onboarding new
employees." }

Response:

{ "answer": "Onboarding best practices include..." }

4.  Agentic RAG Query

To run an agentic RAG workflow, visit:

POST http://localhost:8000/agents/run

Request:

{ "question": "What are the benefits of remote work?" }

Response:

{ "question": "What are the benefits of remote work?", "answer": "Remote
work offers several benefits such as increased flexibility, reduced
commuting time, and improved work-life balance..." }

5.  Ingest Text Manually

If you have raw text data and want to ingest it directly into the system
without file uploads, visit:

POST http://localhost:8000/ingest/text

Request:

{ "text": "The quick brown fox jumps over the lazy dog." }

Conclusion

This project sets up a full Agentic RAG pipeline, integrating multiple
services like CrewAI, pgvector, LlamaIndex, Ollama, and Docling to allow
efficient, model-driven query generation and response management. You
can extend the agents, modify the LlamaIndex configuration, or swap out
models with Ollama as needed.
